### YamlMime:FAQ
metadata:
  title: Document Intelligence (formerly Form Recognizer) frequently asked questions (FAQ)
  description: Frequently asked questions about Document Intelligence service.
  author: laujan
  manager: nitinme
  ms.service: applied-ai-services
  ms.subservice: forms-recognizer
  ms.topic: faq
  ms.date: 08/02/2023
  ms.author: lajanuar
  monikerRange: '<=doc-intel-3.1.0'

title: Frequently asked questions (FAQ)
summary: |
   [!INCLUDE [applies to v3.1, v3.0, and v2.1](includes/applies-to-v3-1-v3-0-v2-1.md)]



sections:
  - name: General concepts
    questions:
      - question: |
          What is Azure AI Document Intelligence and what happened to Form Recognizer?
        answer: |

          - Form Recognizer is now Azure AI Document Intelligence!

          - Azure AI Document Intelligence is a cloud-based Azure AI service that uses machine-learning models to extract key-value pairs, text, and tables from your documents. The returned result is a structured JSON output.

          - Document Intelligence use cases include automated data processing, enhanced data-driven strategies, and enriched document search capabilities.

          - As of July 2023, Azure AI services encompass all of what were previously known as Azure Cognitive Services and Azure Applied AI Services. There are no changes to pricing. The names "Cognitive Services" and "Azure Applied AI" continue to be used in Azure billing, cost analysis, price list, and price APIs. There are no breaking changes to application programming interfaces (APIs) or SDKs.

      - question: |
          How is Document Intelligence related to Document Generative AI?
        answer: |

          A Document Generative AI solution can enable you to chat with your documents, generate captivating content from them and access the power of Azure OpenAI models on your data. With Azure AI Document Intelligence and Azure OpenAI combined, you can build an enterprise application to seamlessly interact with your documents using natural languages, easily find answers and gain valuable insights, effortlessly generate new and engaging content from your existing documents. Check for more details in the [technical community blog](https://techcommunity.microsoft.com/t5/azure-ai-services-blog/document-generative-ai-the-power-of-azure-ai-document/ba-p/3875015).
      
      - question: |
          Which Document Intelligence use cases require special consideration?
        answer: |

          Give careful consideration to document processing projects that encompass financial, protected health, personal identity, or highly sensitive data.

          Make certain to comply with all [national/regional and industry-specific requirements](https://azure.microsoft.com/resources/microsoft-azure-compliance-offerings/).

      - question: |
          What languages does Document Intelligence support?
        answer: |

           Document Intelligence's deep-learning-based universal models support many languages that can extract multi-lingual text from your images and documents, including text lines with mixed languages.

           Language support varies by Document Intelligence service functionality. See [language support](language-support.md) for a complete list of the handwritten and printed text supported by Document Intelligence.

      - question: |
         Is Document Intelligence available in my Azure region?
        answer: |

          Document Intelligence is generally available in many of the [60+ Azure global infrastructure regions](https://azure.microsoft.com/global-infrastructure/services/?products=metrics-advisor&regions=all#select-product).

          [Choose the region](https://azure.microsoft.com/global-infrastructure/geographies/#overview) that is best for you and your customers.

      - question: |
          Does Document Intelligence integrate with other Microsoft services?
        answer: |

          Yes, Document Intelligence integrates with the following services:

          - [AI Builder workflows](/ai-builder/create-form-processing-model?toc=~/articles/ai-services/document-intelligence/toc.json&bc=~/articles/ai-services/document-intelligence/breadcrumb/toc.json)

          - [Azure Cognitive Search](../../search/cognitive-search-custom-skill-form.md)

          - [Azure Functions](tutorial-azure-function.md)

          - [Azure Logic Apps](tutorial-logic-apps.md)

      - question: |
          How is Document Intelligence related to OCR?
        answer: |

          Azure AI Document Intelligence is a cloud-based Azure AI service that is built using optical character recognition (OCR), Text Analytics, and Custom Text from Azure AI services.

          OCR is used to extract typeface and handwritten text documents.

          Document Intelligence uses OCR to detect and extract information from forms and documents supported by AI to provide more structure and information to the text extraction.

      - question: |
          How long is my custom model available for use?
        answer: |

          Models have the same lifecycle as the API version used to train the model. Custom models trained with a generally available (GA) version of the API have the same lifecycle as the API version. When the API version is deprecated, the model is no longer available for inference. Models trained with a preview version of the API also have the same lifecycle as the preview API.

          Expect preview API deprecation within three months of an updated preview API version or newer generally available API version.

      - question: |
           What is the accuracy score and how is it calculated?
        answer: |
           The output of a `build` (v3.0) or `train` (v2.1) custom model operation includes the estimated accuracy score. This score represents the model's ability to accurately predict the labeled value on a visually similar document.

           Accuracy is measured within a percentage value range between 0% (low) and 100% (high).

           See [Interpret and improve accuracy and confidence scores](concept-accuracy-confidence.md#accuracy-scores)

      - question: |
          How can I improve accuracy scores?
        answer: |
          Variances in the visual structure of your documents can influence the accuracy of a model.

          - Ensure that all variations of a document are included in the training dataset. Variations include different formats, for example, digital versus scanned PDFs.

          - Separate visually distinct document types and train different models.

          - Make sure that you don't have extraneous labels.

          - For signature and region labeling, don't include the surrounding text.

          See [Interpret and improve accuracy and confidence scores](concept-accuracy-confidence.md#ensure-high-model-accuracy)

      - question: |
           What is the confidence score and how is it calculated?
        answer: |

           A confidence score indicates probability by measuring the degree of statistical certainty that the extracted result has been detected correctly.

           The confidence value range is a percentage between 0% (low) and 100% (high).

           It's best to target a score of 80% or higher. For more sensitive cases, like financial or medical records, a score of close to 100% is recommended. You may also require human review.

           See [Interpret and improve accuracy and confidence scores](concept-accuracy-confidence.md#confidence-scores)

      - question: |
          How can I improve confidence scores?
        answer: |

         Following an analysis operation, review the JSON output. Examine the `confidence` values for each key/value result under the `pageResults` node. You should also look at the confidence score in the `readResults` node, which corresponds to the text-read operation. The confidence of the read results doesnâ€™t affect the confidence of the key/value extraction results, so you should check both.

            - If the confidence score for the `readResults` object is low, improve the quality of your input documents.

            - If the confidence score for the `pageResults` object is low, ensure that the documents being analyzed are of the same type.

            - Consider incorporating human review into your workflows.

            - Use forms with different values in each field.

            - For custom models, use a larger set of training documents. Tagging more documents teaches your model to recognize fields with greater accuracy.

            See [Interpret and improve accuracy and confidence scores](concept-accuracy-confidence.md#confidence-scores)

      - question: |
          What is a bounding box?
        answer: |

          A bounding box is an abstract rectangle that surrounds text elements on a document or form.  It's used as a reference point for object detection.

          - The bounding box specifies position using an x and y coordinate plane presented in an array of four numerical pairs. Each pair represents a corner of the box in the following order: top-left, top-right, bottom-right, bottom-left.

          - For an image, coordinates are given in pixels.

          - For a PDF, coordinates are given in inches.

      - question: |
          Can Document Intelligence help me classify documents?
        answer: |

          Document Intelligence provides custom classification models that can analyze single or multi-file documents to identify if any of the trained document types are contained within an input file. It supports following scenarios:

          - A single file containing one document type, such as a loan application form

          - A single file containing multiple documents. For instance, a loan application package containing a loan application form, payslip, and bank statement.

          - A single file containing multiple instances of the same document. For instance, a collection of scanned invoices.

          See more details on [Custom classification model](concept-custom-classifier.md).

  - name: App development
    questions:
      - question: |
          What are the development options for Document Intelligence
        answer: |

          Document Intelligence offers the latest development options within the following platforms:

          - [REST API](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-2023-07-31/operations/AnalyzeDocument)

          - [Document Intelligence Studio](https://formrecognizer.appliedai.azure.com/studio)

          - [C#/.NET](https://www.nuget.org/packages/Azure.AI.FormRecognizer/4.0.0)

          - [Java](https://oss.sonatype.org/#nexus-search; quick-azure-ai-formrecognizer)

          - [JavaScript/TypeScript](https://www.npmjs.com/package/@azure/ai-form-recognizer/v/4.0.0-beta.4)

          - [Python](https://pypi.org/project/azure-ai-formrecognizer/3.2.0/)

      - question: |
          Where can I find the supported API version for the latest programming language SDKs?
        answer:   |

          This table provides links to the latest SDK versions and shows the relationship between supported Document Intelligence SDK and API versions: |
            | Supported Language | Azure SDK reference|Supported API version|
            | ----- | -----|-----|
            | C#/.NET| [4.0.0](https://azuresdkdocs.blob.core.windows.net/$web/dotnet/Azure.AI.FormRecognizer/4.0.0/index.html)|[**v3.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-2023-07-31/operations/AnalyzeDocument)</br> [**v2.1**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2-1/operations/AnalyzeBusinessCardAsync)</br>[**v2.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2/operations/AnalyzeLayoutAsync) |
            | Java | [4.0.0](https://azuresdkdocs.blob.core.windows.net/$web/java/azure-ai-formrecognizer/4.0.0/index.html)| [**v3.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-2023-07-31/operations/AnalyzeDocument)</br> [**v2.1**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2-1/operations/AnalyzeBusinessCardAsync)</br>[**v2.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2/operations/AnalyzeLayoutAsync) |
            | JavaScript | [4.0.0](https://azuresdkdocs.blob.core.windows.net/$web/javascript/azure-ai-form-recognizer/4.0.0/index.html)|[**v3.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-2023-07-31/operations/AnalyzeDocument)</br> [**v2.1**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2-1/operations/AnalyzeBusinessCardAsync)</br>[**v2.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2/operations/AnalyzeLayoutAsync) |
            | Python | [3.2.0](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-ai-formrecognizer/3.2.0/index.html) |[**v3.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-2023-07-31/operations/AnalyzeDocument)</br> [**v2.1**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2-1/operations/AnalyzeBusinessCardAsync)</br>[**v2.0**](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2/operations/AnalyzeLayoutAsync) |

           For more information, *see* [Supported clients](sdk-overview.md#supported-clients)


      - question: |
           What is the difference between Document Intelligence v3.0 and v2.1 and how do I migrate to the latest version?
        answer:  |

          For improved usability, Document Intelligence v3.0 introduces a fully redesigned client library. To successfully use the latest Document Intelligence API features, the most recent SDK is required, and your application code must be updated to use the new clients.

          This table provides links to detailed instructions for migrating to the newest version of Document Intelligence:

            | Language / API | Migration guide|
            | ----------- | ----------------  |
            | REST API    | [v3](v3-1-migration-guide.md)|
            | C#/.NET| [4.0.0](https://github.com/Azure/azure-sdk-for-net/blob/Azure.AI.FormRecognizer_4.0.0/sdk/formrecognizer/Azure.AI.FormRecognizer/MigrationGuide.md)|
            | Java | [4.0.0](https://github.com/Azure/azure-sdk-for-java/blob/azure-ai-formrecognizer_4.0.0/sdk/formrecognizer/azure-ai-formrecognizer/migration-guide.md)|
            | JavaScript| [4.0.0](https://github.com/Azure/azure-sdk-for-java/blob/azure-ai-formrecognizer_4.0.0/sdk/formrecognizer/azure-ai-formrecognizer/migration-guide.md)|
            | Python | [3.2.0](https://github.com/Azure/azure-sdk-for-python/blob/azure-ai-formrecognizer_3.2.0/sdk/formrecognizer/azure-ai-formrecognizer/MIGRATION_GUIDE.md)|

      - question: |
            Which file formats does Document Intelligence support? Are there size limitations for input documents?
        answer:  |

            To ensure the best results, see [input requirements](concept-model-overview.md#input-requirements).

      - question: |
            How can I specify a specific range of pages to be analyzed in a document?
        answer:  |

           - The parameter `pages`(supported in both v2.1 and v3.0 REST API) enables you to specify pages for multi-page PDF and TIFF documents. Accepted input includes the following ranges:

              - Single pages (for example,'1, 2' -> pages 1 and 2 are processed).- Finite (for example '2-5' -> pages 2 to 5 are processed)
              - Open-ended ranges (for example '5-' -> all the pages from page 5 are processed & for example, '-10' -> pages 1 to 10 are processed).

            - These parameters can be mixed together and ranges are allowed to overlap (for example, '-5, 1, 3, 5-10' - pages 1 to 10 are processed).

            - The service accepts the request if it can process at least one page of the document. For example, using '5-100' on a five page document is a valid input where page 5 is processed.

            - If no page range is provided, the entire document is processed.

      - question: |
            Both Document Intelligence Studio and the FOTT Sample Labeling tool are available. Which one should I use?
        answer:  |
           Most of time [Document Intelligence Studio]() is recommended since it can reduce your time for configuring Document Intelligence resource and storage services.

           Consider using FOTT (Form OCR Testing Tool) for the following scenarios:

              - Your data must remain within a single machine. Use the [FOTT Sample Labeling tool]() and [Document Intelligence container]().

              - Your project is highly dependent on [Document Intelligence v2.1](https://westus.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2-1/operations/AnalyzeWithCustomForm) and you intend to continue using the v2.1 APIs.

  - name: Service limits and pricing
    questions:
      - question: |
         How does Azure calculate the price for using Document Intelligence services?
        answer: |
         Document Intelligence billing is  calculated monthly based on the model type and number of pages analyzed:

          - When you submit a document for analysis, all pages are analyzed unless you specify a page range with the `pages` parameter in your request. When the service analyzes Microsoft Excel and PowerPoint documents with the new Read OCR model, each worksheet and slide is counted as one page respectively.

          - When analyzing PDF and TIFF files, each page in the PDF file or each image in the TIFF file is counted as one page with no maximum character limits.

          - When analyzing Microsoft Word and HTML files supported by only the Read model, pages are counted in blocks of 3,000 characters each. For example, if your document contains 7,000 characters, the two pages with 3,000 characters each and one page with 1,000 characters adds up to a total of three pages.

          - In addition, when using the Read model, if your Microsoft Word, Excel, and PowerPoint pages have embedded images, each image is analyzed and counted as a page. Therefore, the total analyzed pages for Microsoft Office documents are equal to the sum of total text pages and total images analyzed. In the previous example if the document contains two embedded images, the total page count in the service output is three text pages plus two images equaling a total of five pages.

          - Training a custom model is always free with Document Intelligence. Youâ€™re only charged when a model is used to analyze a document.

          - Container pricing is the same as cloud service pricing.

          - Document Intelligence offers a free tier (F0) that enables you to test all the Document Intelligence features.

          - Document Intelligence has a commitment-based pricing model for large workloads.

         Learn more about [Azure AI Document Intelligence pricing options](https://azure.microsoft.com/pricing/details/form-recognizer/#pricing).

      - question: |
         How can I check my Document Intelligence usage and estimate the price?
        answer: |
          You can find usage metrics in the Azure portal metrics dashboard. The dashboard displays the number of pages processed by Azure AI Document Intelligence. You can check the estimated cost spent on the resource using the [Azure pricing calculator](https://azure.microsoft.com/pricing/calculator/). For detailed instructions, *see* [Check my usage and estimate the cost](how-to-guides/estimate-cost.md).

      - question: |
          What are best practices to mitigate throttling?
        answer: |
          Document Intelligence uses autoscaling to provide the required computational resources on-demand, while keeping customer costs low. To mitigate throttling during autoscaling, we recommend the following approach:

          - Implement retry logic in your application.

          - If you find that youâ€™re being throttled on the number of POST requests, consider adding a delay between the requests.

          - Increase the workload gradually. Avoid sharp changes.

          - [Create a support request](service-limits.md#create-and-submit-support-request) to increase transactions per second(TPS) limit.

          Learn more about Document Intelligence [service quotas and limits](service-limits.md)

      - question: |
         How long does it take to analyze a document?
        answer: |
          Document Intelligence is a multi-tenanted service where latency for similar documents is comparable but not always identical. The time to analyze a document depends on the size (for example, number of pages) and associated content on each page.

          Latency is the amount of time it takes for an API server to handle and process an incoming request and deliver the outgoing response to the client. Occasional variability in latency and performance is inherent in any micro-service-based, stateless, asynchronous service that processes images and large documents at scale. While we're continuously scaling up the hardware and capacity and scaling capabilities, you may still see latency issues at run time.

  - name: Custom models
    questions:
      - question: |
          How do I assemble the best training data?
        answer: |
          When you use the Document Intelligence custom model, you provide your own training data. Here are a few tips to help train your models effectively:

            - Use text-based instead of image-based PDFs when possible. One way to identify an image-based PDF is to try selecting specific text in the document. If you can only select the entire image of the text, the document is image-based, not text-based.

            - Organize your training documents by using a subfolder for each format (JPEG/JPG, PNG, BMP, PDF, or TIFF).

            - Use forms that have all of the available fields completed.

            - Use forms with differing values in each field.

            - If your images are low quality, use a larger data set (more than five training documents).

            Learn more about [building a training data set](how-to-guides/build-a-custom-model.md?view=doc-intel-2.1.0&preserve-view=true)

      - question: |
          What are best practices for training a highly accurate custom model?
        answer: |
         The level of accuracy for your model is dependent on the quality of your training materials:

          - Determine if you need to use a single model or multiple models composed into a single model.

          - Model accuracy can decrease when you have different formats analyzed with a single model. Plan on segmenting your dataset into folders, where each folder is a unique template. Train one model per folder and compose the resulting models into a single endpoint.

          - Custom forms rely on a consistent visual template. If your form has variations with formats and page breaks, consider segmenting your dataset to train multiple models.

          - Ensure you have a balanced dataset by accounting for formats, document types, and structure.

          Learn more about [composed models](concept-custom.md).

      - question: |
          Can I retrain a custom model?
        answer: |

          Document Intelligence doesnâ€™t have an explicit retrain operation. Each train operation generates a new model.

          If you find that your model needs retraining, add more samples to your training data set and train a new model.

      - question: |
          How many custom models can I compose into a single custom model?
        answer: |
           With the [Model Compose](how-to-guides/compose-custom-models.md?view=doc-intel-2.1.0&preserve-view=true#create-a-composed-model) operation, you can assign up to 200 models to a single model ID.

           - When you make the Analyze request with a composed modelID, Document Intelligence classifies the submitted form, chooses the best model, and returns the results.

           - Model Compose is currently available only for custom models trained with labels.

           - Analyzing a document with composed models is identical to analyzing a document with a single model, the analyze result returns a ```docType``` property indicating which of the component models was selected for analyzing the document. There's no change in pricing for analyzing a document with an individual custom model or a composed custom model.

           Learn more about [composed models](concept-custom.md).

      - question: |
          If the number of models I want to compose exceeds the upper limit of composed model, what are the alternatives?
        answer: |
           You can classify the documents before calling the custom model or consider [Custom neural model](concept-custom-neural.md):

           - Use [Read model](concept-read.md) and build a classification based on the extracted text from the documents and certain phrases using code, regular expressions, search etc.

           - If you want to extract the same fields from various structured, semi-structured, and unstructured documents. Consider using the deep learning [custom neural model](concept-custom-neural.md). Learn more about the [differences between custom template model and custom neural model](concept-custom.md#compare-model-features).

      - question: |
          How do I refine a model beyond the initial training?
        answer: |
         Each train operation generates a new model.

          - Start by creating a dataset for your new template.

          - Label and train a new model.

          - Validate that the new model performs well for your specific document types.

          - Compose your new model with the existing model into a single endpoint. Document Intelligence can then determine the best model for each document to be analyzed.

          Learn more about [composed models](concept-custom.md).

      - question: |
            I'm building a custom model, what does the signature-detection label return?
        answer: |
            [Signature detection](quickstarts/try-document-intelligence-studio.md#signature-detection) looks for the presence of a signature, not the identity of the person signing the document.

            If the model returns "unsigned" for signature detection, the model didnâ€™t find a signature in the defined field.

      - question: |
             What should I consider and what are best practices for extracting tables from documents?
        answer: |
           You can start with the Document Intelligence [layout model](concept-layout.md) to extract texts, tables, selection marks, and structure information from documents and images. You can also consider the following factors:

             - Is the data that you wish to extract presented as a table and is the table structure meaningful?

             - If the data isnâ€™t in a table format, can the data fit in a two-dimensional grid?

             - Do your tables span across multiple pages? If so, to avoid having to label all of the pages, split the PDF into pages prior to sending it to Document Intelligence. Following the analysis, post-process the pages to a single table.

             - If youâ€™re creating custom models, refer to [Labeling as tables](quickstarts/try-document-intelligence-studio.md#labeling-as-tables). Dynamic tables have a variable number of rows for each given column. Fixed tables have a constant number of rows for each given column.

      - question: |
            How can I move my trained models from one environment (like beta) to another (like production)?
        answer: |
            The Copy API enables this scenario by allowing you to copy custom models from one Document Intelligence account or into others, which can exist in any supported geographical region. Follow [this document](disaster-recovery.md) for detailed instructions. The copy operation is limited to copying models within the specific cloud environment the model was trained in. For instance, copying models from the public cloud to the Azure Government clod isn't supported.

  - name: Storage account
    questions:
      - question: |
         I was able to access my storage account a few days ago. Why am I now having trouble reconnecting?
        answer:  |

          When you create a shared access signature (SAS), the default duration is 48 hours. After 48 hours, you'll need to create a new token.

          Consider setting a longer duration period for the time you're using your storage account with Document Intelligence.

      - question: |
           If my storage account is behind a VNet or firewall, how do I give Document Intelligence access to my storage account data?
        answer: |
            If you have an Azure storage account protected by a Virtual Network (VNet) or firewall, Document Intelligence canâ€™t directly access your storage account. However, Private Azure storage account access and authentication support [managed identities for Azure resources](../../active-directory/managed-identities-azure-resources/overview.md). Once a managed identity is enabled, the Document Intelligence service can access your storage account using an assigned managed identity credential.

            If you intend to analyze your private storage account data with FOTT, the tool must be deployed behind the VNet or firewall.

            Learn how to [create and use a managed identity for your Document Intelligence resource](managed-identities.md)

  - name: Document Intelligence Studio
    questions:
      - question: |
          What permissions do I need to access Document Intelligence Studio?
        answer: |

          - You need an active [Azure account](https://azure.microsoft.com/free/cognitive-services/) and subscription with at least a **Reader** role to access Document Intelligence Studio.

          - For **document analysis and prebuilt models**, you need full accessâ€”**Contributor** roleâ€”to at least one [Document Intelligence](https://portal.azure.com/#create/Microsoft.CognitiveServicesFormRecognizer) or [multi-service](https://portal.azure.com/#create/Microsoft.CognitiveServicesAllInOne) resource to enter the analyze page. Once you access the model analyze page, you can change the endpoint and key to access other resources, if needed.

          - For **custom models**, you can either use a **Contributor** role, or use the endpoint and key of a [Document Intelligence](https://portal.azure.com/#create/Microsoft.CognitiveServicesFormRecognizer) or [multi-service](https://portal.azure.com/#create/Microsoft.CognitiveServicesAllInOne) resource to create a project. You also need to have **Contributor** role to access to at least one blob storage account.

          - For more information, *see* [Azure AD built-in roles](../../role-based-access-control/built-in-roles.md).

      - question: |
          I have multiple pages in a document. Why are there only two pages analyzed in Document Intelligence Studio?
        answer: |

          For free (F0) tier resources, only the first two pages are analyzed no matter you're using Document Intelligence Studio, REST API or SDKs. In Document Intelligence Studio, select the top right gear button (Settings), choose the Resources tab and check the Price Tier you're using to analyze the documents. Change to an S0 paid resource if you want to analyze all pages in a document.

      - question: |
          How can I change directories or subscriptions to use in Document Intelligence Studio?
        answer: |

          - In Document Intelligence Studio, you can select the top right gear button (Settings), under Directory, search and select the directory from the list and select on Switch Directory. You'll be prompted to sign in again after switching directory.

          - Switching subscriptions or resources can be done under Settings -> Resource tab.

      - question: |
          Why am I receiving an AuthorizationFailure error on Project Sharing, Auto Label, or OCR Upgrade when my Document Intelligence or Storage Account resource is configured with a firewall?
        answer: |

          Add our website IP address, 20.3.165.95, to the firewall allowlist for both Document Intelligence and Storage Account resources. This is Document Intelligence Studio's dedicated IP address and can be safely allowed.

      - question: |
          Can I reuse or customize the labeling experience from Studio and build it into my own application?
        answer: |

          Yes. The labeling experience from Studio is open sourced in the [Toolkit repo](https://github.com/microsoft/Form-Recognizer-Toolkit)


  - name: Containers
    questions:
      - question: |
          Do I need an internet connection to use Document Intelligence containers?
        answer: |

          Yes, Document Intelligence containers require internet connectivity to send [billing information](containers/install-run.md?tabs=layout#billing) to Azure.
          Learn more about [Azure container security](../../container-instances/container-instances-image-security.md#security-recommendations-for-azure-container-instances).

      - question: |
          What's the difference between disconnected and connected container?
        answer: |

          - Document Intelligence connected containers send billing information to Azure using a Document Intelligence resource on your Azure account. With connected containers, internet connectivity is required to send [billing information](containers/install-run.md?tabs=layout#billing) to Azure.

          - [Disconnected containers](../../ai-services/containers/disconnected-containers.md) enable you to use APIs disconnected from the internet. [Billing information](../../ai-services/containers/disconnected-container-faq.yml#how-does-billing-work) isn't sent via the internet, instead you're charged based upon a purchased commitment tier. Currently, disconnected container usage is available for Document Intelligence custom and invoice models. The model capabilities provided in connected and disconnected containers are the same and supported by Document Intelligence v2.1.

      - question: |
          What data do connected containers send to the cloud?
        answer: |

          Document Intelligence connected containers send billing information to Azure by using a Document Intelligence resource on your Azure account. Connected containers don't send customer data, such as the image or text that's being analyzed, to Microsoft. See the [Azure AI container FAQ](../../ai-services/containers/disconnected-container-faq.yml#how-does-billing-work) for an example of the information sent to Microsoft for billing.

      - question: |
           I received an "OutOfQuota" error message: *Container isn't in a valid state. Subscription validation failed with status 'OutOfQuota' API key is out of quota*.
        answer: |

          Document Intelligence connected containers send billing information to Azure by using a Document Intelligence resource on your Azure account. You could get this message if the containers can't communicate with the billing endpoint.

      - question: |
          Can I use local storage for the Document Intelligence Sample Labeling Tool (FOTT) container?
        answer: |

          FOTT has a version that uses local storage. The version needs to be installed on a Windows machine. You can install it from [this location](https://github.com/microsoft/OCR-Form-Tools/releases/download/v2.1-ga/oflt-2.1.3-win32.exe). On the project page, specify the Label folder URI as /shared or /shared/sub-dir if your labeling files are in a sub directory. All other Document Intelligence Sample Labeling Tool behavior is the same as the hosted service.

  - name: Security and Privacy
    questions:
      - question: |
          What are the different methods and requirements for authenticating requests to Azure AI services?
        answer: |
          Each request to an Azure service must include an authentication header. You can authenticate a request with several methods:

          - [Authenticate](create-document-intelligence-resource.md?#get-endpoint-url-and-keys) with a [single-service](https://portal.azure.com/#create/Microsoft.CognitiveServicesFormRecognizer) or [multi-service](https://portal.azure.com/#create/Microsoft.CognitiveServicesAllInOne) key.

          - [Authenticate](../../ai-services/authentication.md?context=%2fazure%2fapplied-ai-services%2fform-recognizer%2fcontext%2fcontext&tabs=PowerShell#authenticate-with-azure-active-directory) with Azure Active Directory (Azure AD).

          - [Enable](./encrypt-data-at-rest.md#enable-customer-managed-keys-for-your-resource) customer-managed keys.

          - [Authorize](managed-identities.md#enable-a-system-assigned-managed-identity) managed identities.

      - question: |
          Does Document Intelligence store my data?
        answer: |
          For all features, Document Intelligence temporarily stores data and results in Azure storage in the same region as the request. Your data is then deleted within 24 hours from the time an analyze request was submitted.

          Learn more about [Data, privacy, and security for Document Intelligence](/legal/cognitive-services/document-intelligence/data-privacy-security?toc=/azure/ai-services/document-intelligence/toc.json&bc=/azure/ai-services/document-intelligence/breadcrumb/toc.json).

      - question: |
          How are my trained custom models stored and utilized in Document Intelligence?
        answer: |
          The Custom model feature allows customers to build custom models from training data stored in customerâ€™s Azure blob storage locations. The interim outputs after analysis and labeling are stored in the same location. The trained custom models are stored in Azure storage in the same region and logically isolated with their Azure subscription and API credentials.

  - name: More help and support
    questions:
      - question: |
            Where can I find more solutions to my Azure AI Document Intelligence questions?
        answer: |
             [Microsoft Q & A](/answers/topics/azure-form-recognizer.html) is the home for technical questions and answers at Microsoft. You can filter queries specific to Document Intelligence.

      - question: |
            What should I do if specific text isnâ€™t recognized or recognized incorrectly when labeling documents?
        answer: |
            We continually update and improve the Document Intelligence OCR model. You can reach out to the Document Intelligence team: [formrecog_contact@microsoft.com](mailto:formrecog_contact@microsoft.com). If possible, share a sample document with the issue highlighted.
